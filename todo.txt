Next Steps:

- try to optimize a class probability in the latent space of a trained AE.
- write a single script that achieves the following:
    - train a classifier
    - for some depths, train an autoencoder
    - save everything to disk --> probably no structure beyond checkpoint filenames?
- write code to perform label-informed data augmentation through autoencoders (and not) 
  during classifier training --> think how to design this; should be modular and reusable



More:

- debug why no checkpoint is saved apart from last.ckpt even when save_top_k > 1
- can improve:
    - statistics about latent space
    - sampling around an embedding in latent space exploration (more options, e.g. sample at multiple
      gaussian noise levels or do a MCMC guided by probas)
- implement other autoencoder variants (e.g. VAE, VQVAE)
- to understand hidden layer perturbations (of any kind, including through a perturbation in the latent space on an AE),
  setup the following: given a dataset and a classifier, collect all hidden representations at that level of the datapoints.
  then, do nearest neighbor queries to interpret the 'moves' in hidden representation space.
