Next Steps:

- put comparison of flatness around genuine examples and adversarial examples in a script
- run exploration of latent space, input gradients, and genuine vs adversarial for tiny imagenet
- what can we do with multiple clasifier seeds? 
  - are adversarial examples shared?
  - can we find shared adversarial examples?
  - can we guide the search in input space with many classifiers?
- train a better autoencoder!
  - variational?
  - more constrained bottleneck?


Suspended for now...
  - improve augmentation:
    - have a probability < 1 of doing the augmentation at train time
    - condition it on the classification being correct
    - solve TODOs in-code
  - train a classifier and autoencoders on its representations
    - focus on input layer first
      - here easy interpretation; log images conditionally on layer_idx == 0!
      - use existing machinery to interpret the augmentation
      - search for a way to do it that improves performance?
    - add logging to judge quality of autoencoders in an interpretable way for hidden layers (e.g. look at reconstruction error vs typical magnitude or typical variation across dataset)


More:

- improve consistency of conventions (e.g. paths handling, docstrings, tensor manipulations, ...)
- refactor flatness code (make modular?)
- when optimizing proba constrained on the AE manifold, inject noise in classifier?
  - similar to SGD noise in analogy with weight landscape (face a noisy sample from data distro at every iteration,
    here e.g. face a classifier from a ball around a reference one)
  - flatness/sharpness?
    - adversarial vs genuine examples
    - optimizing (adversarial) with/without noise as described above
  - mode connectivity in input space?
    - adversarial examples different from genuine?
- can improve:
    - statistics about latent space
    - sampling around an embedding in latent space exploration (more options, e.g. sample at multiple
      gaussian noise levels or do a MCMC guided by probas)
- implement other autoencoder variants (e.g. VAE, VQVAE)
- to understand hidden layer perturbations (of any kind, including through a perturbation in the latent space on an AE),
  setup the following: given a dataset and a classifier, collect all hidden representations at that level of the datapoints.
  then, do nearest neighbor queries to interpret the 'moves' in hidden representation space.
- input landscape (noise in optimization, flatness, mode connectivity, ...)