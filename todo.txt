Next Steps:

- integrate plot_average_input_flatness in plot_inputs_flatness in the module flatness.py
- compare flatness around genuine examples and adversarial examples


- Once HPC is back:
  - run explore gradients with finetuned resnet18 on tiny imagenet
  - different classifiers: adversarial examples shared?
- input landscape (noise in optimization, flatness, mode connectivity, ...)
  - compute input flatness for a few examples (genuine images, adversarial images, changing the optimization recipe (e.g. SGD, noise), etc)


Suspended for now...
  - improve augmentation:
    - have a probability < 1 of doing the augmentation at train time
    - condition it on the classification being correct
    - solve TODOs in-code
  - train a classifier and autoencoders on its representations
    - focus on input layer first
      - here easy interpretation; log images conditionally on layer_idx == 0!
      - use existing machinery to interpret the augmentation
      - search for a way to do it that improves performance?
    - add logging to judge quality of autoencoders in an interpretable way for hidden layers (e.g. look at reconstruction error vs typical magnitude or typical variation across dataset)


More:

- improve consistency of conventions (e.g. paths handling, docstrings, tensor manipulations, ...)
- refactor flatness code (make modular?)
- when optimizing proba constrained on the AE manifold, inject noise in classifier?
  - similar to SGD noise in analogy with weight landscape (face a noisy sample from data distro at every iteration,
    here e.g. face a classifier from a ball around a reference one)
  - flatness/sharpness?
    - adversarial vs genuine examples
    - optimizing (adversarial) with/without noise as described above
  - mode connectivity in input space?
    - adversarial examples different from genuine?
- can improve:
    - statistics about latent space
    - sampling around an embedding in latent space exploration (more options, e.g. sample at multiple
      gaussian noise levels or do a MCMC guided by probas)
- implement other autoencoder variants (e.g. VAE, VQVAE)
- to understand hidden layer perturbations (of any kind, including through a perturbation in the latent space on an AE),
  setup the following: given a dataset and a classifier, collect all hidden representations at that level of the datapoints.
  then, do nearest neighbor queries to interpret the 'moves' in hidden representation space.